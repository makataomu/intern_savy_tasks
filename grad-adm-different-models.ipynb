{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-22T09:19:26.284045Z","iopub.execute_input":"2023-04-22T09:19:26.284552Z","iopub.status.idle":"2023-04-22T09:19:26.297006Z","shell.execute_reply.started":"2023-04-22T09:19:26.284515Z","shell.execute_reply":"2023-04-22T09:19:26.295425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Understanding data:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:26.299831Z","iopub.execute_input":"2023-04-22T09:19:26.300375Z","iopub.status.idle":"2023-04-22T09:19:26.309374Z","shell.execute_reply.started":"2023-04-22T09:19:26.300301Z","shell.execute_reply":"2023-04-22T09:19:26.307988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ap = pd.read_csv(\"/kaggle/input/graduate-admissions/Admission_Predict_Ver1.1.csv\")\nap.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:26.311616Z","iopub.execute_input":"2023-04-22T09:19:26.312018Z","iopub.status.idle":"2023-04-22T09:19:26.338805Z","shell.execute_reply.started":"2023-04-22T09:19:26.311982Z","shell.execute_reply":"2023-04-22T09:19:26.337411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ap.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:26.341372Z","iopub.execute_input":"2023-04-22T09:19:26.342748Z","iopub.status.idle":"2023-04-22T09:19:26.350561Z","shell.execute_reply.started":"2023-04-22T09:19:26.342695Z","shell.execute_reply":"2023-04-22T09:19:26.349497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's check all values\nap.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:26.392096Z","iopub.execute_input":"2023-04-22T09:19:26.392739Z","iopub.status.idle":"2023-04-22T09:19:26.405676Z","shell.execute_reply.started":"2023-04-22T09:19:26.392703Z","shell.execute_reply":"2023-04-22T09:19:26.404754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fancy stuff\nap.hist(figsize=(14, 10))\nNone","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:26.424172Z","iopub.execute_input":"2023-04-22T09:19:26.425366Z","iopub.status.idle":"2023-04-22T09:19:27.940009Z","shell.execute_reply.started":"2023-04-22T09:19:26.425305Z","shell.execute_reply":"2023-04-22T09:19:27.938445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Select features:","metadata":{}},{"cell_type":"code","source":"num_cols = [\n    'GRE Score',\n    'TOEFL Score',\n    'University Rating',\n    'SOP',\n    'LOR',\n    'CGPA',\n    'Research'\n]\n\ntarget_col = 'Chance of Admit'\n\nap.columns = ap.columns.to_series().apply(lambda x: x.strip())\n\ncols = num_cols + [target_col]\nap = ap[cols]","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:27.942205Z","iopub.execute_input":"2023-04-22T09:19:27.943528Z","iopub.status.idle":"2023-04-22T09:19:27.953470Z","shell.execute_reply.started":"2023-04-22T09:19:27.943472Z","shell.execute_reply":"2023-04-22T09:19:27.952130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Further Analysis","metadata":{}},{"cell_type":"code","source":"ap.describe()","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:27.955047Z","iopub.execute_input":"2023-04-22T09:19:27.955932Z","iopub.status.idle":"2023-04-22T09:19:27.999433Z","shell.execute_reply.started":"2023-04-22T09:19:27.955892Z","shell.execute_reply":"2023-04-22T09:19:27.998451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ap.corr().style.background_gradient(cmap='coolwarm').set_precision(2)\nNone","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:28.002332Z","iopub.execute_input":"2023-04-22T09:19:28.002709Z","iopub.status.idle":"2023-04-22T09:19:28.011572Z","shell.execute_reply.started":"2023-04-22T09:19:28.002676Z","shell.execute_reply":"2023-04-22T09:19:28.010703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparations:","metadata":{}},{"cell_type":"code","source":"X = ap.drop('Chance of Admit', axis=1)\ny = ap['Chance of Admit']","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:28.012763Z","iopub.execute_input":"2023-04-22T09:19:28.013639Z","iopub.status.idle":"2023-04-22T09:19:28.021730Z","shell.execute_reply.started":"2023-04-22T09:19:28.013547Z","shell.execute_reply":"2023-04-22T09:19:28.020470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we're splitting the data by 70:15:15 because we'll use validation to find best hyperparameters and later test to evaluate the model.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data into training, validation, and testing sets as 70:15:15\nX_train, X_valtest, y_train, y_valtest = train_test_split(X, y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:28.023168Z","iopub.execute_input":"2023-04-22T09:19:28.023814Z","iopub.status.idle":"2023-04-22T09:19:28.035847Z","shell.execute_reply.started":"2023-04-22T09:19:28.023778Z","shell.execute_reply":"2023-04-22T09:19:28.034604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When preparing your data for machine learning, it is important to split your data into training and test sets *before doing any scaling* or transformation. You should apply any scaling or transformation on the training set only, and then apply the same transformation to the test set using the parameters learned from the training set. This approach ensures that your model is evaluated on unseen data and avoids any data leakage issues.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Fit the scaler on the training set\nscaler.fit(X_train)\n\n# Transform the training, validation, and testing sets\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:28.037188Z","iopub.execute_input":"2023-04-22T09:19:28.037539Z","iopub.status.idle":"2023-04-22T09:19:28.055418Z","shell.execute_reply.started":"2023-04-22T09:19:28.037505Z","shell.execute_reply":"2023-04-22T09:19:28.053751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Linear Regression\nlr = LinearRegression()\nlr.fit(X_train_scaled, y_train)\n\n# Evaluate on validation set\ny_pred = lr.predict(X_val_scaled)\nmse = mean_squared_error(y_val, y_pred)\nr2 = r2_score(y_val, y_pred)\n\nprint(\"Linear Regression\")\nprint(\"MSE val: \", mse)\nprint(\"R2 val score: \", r2)\n\n# Evaluate on test set\ny_pred_test = lr.predict(X_test_scaled)\nmse_test = mean_squared_error(y_test, y_pred_test)\nr2_test = r2_score(y_test, y_pred_test)\n\nprint()\nprint(\"MSE test : \", mse_test)\nprint(\"R2 test score: \", r2_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:28.057284Z","iopub.execute_input":"2023-04-22T09:19:28.057958Z","iopub.status.idle":"2023-04-22T09:19:28.074217Z","shell.execute_reply.started":"2023-04-22T09:19:28.057922Z","shell.execute_reply":"2023-04-22T09:19:28.073130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If the points are *closely* clustered around the diagonal line, it indicates good performance of the model.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Plot actual vs predicted for linear regression\nplt.scatter(y_test, y_pred_test)\nplt.plot([0, 1], [0, 1], '--k')  # diagonal line for reference\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Linear Regression')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:28.075991Z","iopub.execute_input":"2023-04-22T09:19:28.076789Z","iopub.status.idle":"2023-04-22T09:19:28.237132Z","shell.execute_reply.started":"2023-04-22T09:19:28.076740Z","shell.execute_reply":"2023-04-22T09:19:28.236041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Polynomial Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:28.242370Z","iopub.execute_input":"2023-04-22T09:19:28.242970Z","iopub.status.idle":"2023-04-22T09:19:28.248554Z","shell.execute_reply.started":"2023-04-22T09:19:28.242919Z","shell.execute_reply":"2023-04-22T09:19:28.247629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_r2, best_deg = 0, 0\n\nr2_values = []\n\ndegrees = np.arange(1,5)\n\nfor deg in degrees:\n  #poly features\n  poly_features = PolynomialFeatures(degree=deg, include_bias=False)\n  X_train_poly = poly_features.fit_transform(X_train_scaled)\n\n  #linear Regression\n  lr_poly = LinearRegression()\n  lr_poly.fit(X_train_poly, y_train)  \n\n  #test\n  X_val_poly = poly_features.transform(X_val_scaled)\n  y_pred_poly = lr_poly.predict(X_val_poly)\n\n  #mse_poly = mean_squared_error(y_val, y_pred_poly)\n  r2_poly = r2_score(y_val, y_pred_poly)\n  r2_values.append(r2_poly)\n\n  if r2_poly > max_r2:\n    max_r2 = r2_poly\n    best_deg = deg\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(degrees, r2_values)\nax.set_yscale('log')\nax.set_xlabel('Degree')\nax.set_ylabel('R2')","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:28.249864Z","iopub.execute_input":"2023-04-22T09:19:28.250623Z","iopub.status.idle":"2023-04-22T09:19:28.657797Z","shell.execute_reply.started":"2023-04-22T09:19:28.250578Z","shell.execute_reply":"2023-04-22T09:19:28.656498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"degree = best_deg  # degree of the polynomial\npoly = PolynomialFeatures(degree)\nX_train_poly = poly.fit_transform(X_train_scaled)\n\nlr_poly = LinearRegression()\nlr_poly.fit(X_train_poly, y_train)\n\n# Evaluate on validation set\nX_val_poly = poly.transform(X_val_scaled)\ny_pred_poly = lr_poly.predict(X_val_poly)\nmse_poly = mean_squared_error(y_val, y_pred_poly)\nr2_poly = r2_score(y_val, y_pred_poly)\n\nprint(\"Polynomial Regression (degree = {})\".format(degree))\nprint(\"MSE: \", mse_poly)\nprint(\"R2 score: \", r2_poly)\n\n# Evaluate on test set\nX_test_poly = poly.transform(X_test_scaled)\ny_pred_test_poly = lr_poly.predict(X_test_poly)\nmse_poly = mean_squared_error(y_test, y_pred_test_poly)\nr2_poly = r2_score(y_test, y_pred_test_poly)\n\nprint()\nprint(\"MSE test : \", mse_poly)\nprint(\"R2 test score: \", r2_poly)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:28.659525Z","iopub.execute_input":"2023-04-22T09:19:28.660401Z","iopub.status.idle":"2023-04-22T09:19:28.674111Z","shell.execute_reply.started":"2023-04-22T09:19:28.660360Z","shell.execute_reply":"2023-04-22T09:19:28.672974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Actually, results are different from those that I had in Colab. Here results for Linear and Polynomial are the same because the best degree found is 1, therefore those functions are the same.  ","metadata":{}},{"cell_type":"code","source":"# Plot actual vs predicted for polynomial regression\nplt.scatter(y_test, y_pred_test_poly)\nplt.plot([0, 1], [0, 1], '--k')  # diagonal line for reference\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Polynomial Regression (degree = {})'.format(degree))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:28.675741Z","iopub.execute_input":"2023-04-22T09:19:28.676221Z","iopub.status.idle":"2023-04-22T09:19:28.903809Z","shell.execute_reply.started":"2023-04-22T09:19:28.676186Z","shell.execute_reply":"2023-04-22T09:19:28.902862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Multicolinearity Check:","metadata":{}},{"cell_type":"code","source":"corr_matrix = X.corr()\n\n# Display the correlation matrix\nprint(corr_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:28.905068Z","iopub.execute_input":"2023-04-22T09:19:28.906126Z","iopub.status.idle":"2023-04-22T09:19:28.917079Z","shell.execute_reply.started":"2023-04-22T09:19:28.906088Z","shell.execute_reply":"2023-04-22T09:19:28.915691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\n# Create a heatmap of the correlation matrix\nsns.heatmap(corr_matrix, cmap='coolwarm')\nNone","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:28.918852Z","iopub.execute_input":"2023-04-22T09:19:28.920399Z","iopub.status.idle":"2023-04-22T09:19:29.265963Z","shell.execute_reply.started":"2023-04-22T09:19:28.920348Z","shell.execute_reply":"2023-04-22T09:19:29.264743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see there's a *big colinearity* between TOEFL, GRE scores and CGPA. So let's use Lasso, Ridge and ElasticNet Regressions.","metadata":{}},{"cell_type":"markdown","source":"# Lasso Regression:","metadata":{}},{"cell_type":"markdown","source":"Lasso is particularly useful when dealing with high-dimensional datasets with many irrelevant features.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LassoCV\n\n# Define the range of alpha values to try\nalphas = [0.001, 0.01, 0.1, 1]\n\n# Create a LassoCV object with the range of alpha values and set cv=5 for 5-fold cross-validation\nlasso_cv = LassoCV(alphas=alphas, cv=5)\n\n# Fit the LassoCV object to the training data\nlasso_cv.fit(X_train_scaled, y_train)\n\n# Print the best alpha value and corresponding R^2 score\nprint(\"Best alpha:\", lasso_cv.alpha_)\nprint(\"R^2 score with best alpha:\", lasso_cv.score(X_train_scaled, y_train))\n\n# Evaluate the model on the validation set\nval_score = lasso_cv.score(X_val_scaled, y_val)\nprint(\"Validation R^2 score:\", val_score)\n\n# Evaluate the model on the test set\ntest_score = lasso_cv.score(X_test_scaled, y_test)\nprint(\"Test R^2 score:\", test_score)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:29.267907Z","iopub.execute_input":"2023-04-22T09:19:29.268676Z","iopub.status.idle":"2023-04-22T09:19:29.291023Z","shell.execute_reply.started":"2023-04-22T09:19:29.268614Z","shell.execute_reply":"2023-04-22T09:19:29.289565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ridge Regression:","metadata":{}},{"cell_type":"markdown","source":"Ridge is particularly useful when there are many predictors with small or moderate effect sizes. Probably, not our case. ","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\n\n# Define the range of alpha values to try\nalphas = [1, 10, 20, 30]\n\nridge_cv = RidgeCV(alphas=alphas, cv=5)\n\nridge_cv.fit(X_train_scaled, y_train)\n\nprint(\"Best alpha:\", ridge_cv.alpha_)\nprint(\"R^2 score with best alpha:\", ridge_cv.score(X_train_scaled, y_train))\n\nval_score = ridge_cv.score(X_val_scaled, y_val)\nprint(\"Validation R^2 score:\", val_score)\n\ntest_score = ridge_cv.score(X_test_scaled, y_test)\nprint(\"Test R^2 score:\", test_score)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:29.292392Z","iopub.execute_input":"2023-04-22T09:19:29.292719Z","iopub.status.idle":"2023-04-22T09:19:29.347035Z","shell.execute_reply.started":"2023-04-22T09:19:29.292688Z","shell.execute_reply":"2023-04-22T09:19:29.345754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\n\nelastic_net = ElasticNet()\n\n# Set up the parameter grid\nparam_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10],\n              'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]}\n\ngrid_search = GridSearchCV(elastic_net, param_grid, cv=5)\n\ngrid_search.fit(X_train_scaled, y_train)\n\nprint(\"Best alpha value:\", grid_search.best_params_['alpha'])\nprint(\"Best l1_ratio value:\", grid_search.best_params_['l1_ratio'])\n\n# Print R^2 score with best hyperparameters\nelastic_net_best = ElasticNet(alpha=grid_search.best_params_['alpha'], l1_ratio=grid_search.best_params_['l1_ratio'])\nelastic_net_best.fit(X_train_scaled, y_train)\nprint(\"R^2 score with best hyperparameters:\", elastic_net_best.score(X_train_scaled, y_train))\n\nval_score = elastic_net_best.score(X_val_scaled, y_val)\nprint(\"Validation R^2 score:\", val_score)\n\ntest_score = elastic_net_best.score(X_test_scaled, y_test)\nprint(\"Test R^2 score:\", test_score)","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:29.350859Z","iopub.execute_input":"2023-04-22T09:19:29.351191Z","iopub.status.idle":"2023-04-22T09:19:29.599145Z","shell.execute_reply.started":"2023-04-22T09:19:29.351160Z","shell.execute_reply":"2023-04-22T09:19:29.597676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid to search\nparam_grid = {'max_depth': [1, 2, 3, 4, 5],\n              'min_samples_split': [2, 3, 4, 5, 6]}\n\n# Create a DecisionTreeRegressor object\ndt = DecisionTreeRegressor()\n\n# Create a GridSearchCV object with the parameter grid to search and 5-fold cross-validation\ngrid_search = GridSearchCV(dt, param_grid, cv=5)\n\n# Fit the grid search object to the training data\ngrid_search.fit(X_train_scaled, y_train)\n\n# Print the best hyperparameters and R^2 score\nprint(\"Best hyperparameters:\", grid_search.best_params_)\nprint(\"Training R^2 score:\", grid_search.best_score_)\nprint(\"Validation R^2 score:\", grid_search.score(X_val_scaled, y_val))\nprint(\"Test R^2 score:\", grid_search.score(X_test_scaled, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:29.600567Z","iopub.execute_input":"2023-04-22T09:19:29.600976Z","iopub.status.idle":"2023-04-22T09:19:29.974543Z","shell.execute_reply.started":"2023-04-22T09:19:29.600943Z","shell.execute_reply":"2023-04-22T09:19:29.973257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the parameter grid to search\nparam_grid = {'n_estimators': [30, 50, 100],\n              'max_depth': [2, 3, 4, 5],\n              'min_samples_split': [3, 4, 5, 6]}\n\n# Create a RandomForestRegressor object\nrf = RandomForestRegressor()\n\n# Create a GridSearchCV object with the parameter grid to search and 5-fold cross-validation\ngrid_search = GridSearchCV(rf, param_grid, cv=5)\n\n# Fit the grid search object to the training data\ngrid_search.fit(X_train_scaled, y_train)\n\n# Print the best hyperparameters and R^2 score\nprint(\"Best hyperparameters:\", grid_search.best_params_)\nprint(\"Training R^2 score:\", grid_search.best_score_)\nprint(\"Validation R^2 score:\", grid_search.score(X_val_scaled, y_val))\nprint(\"Test R^2 score:\", grid_search.score(X_test_scaled, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:29.976245Z","iopub.execute_input":"2023-04-22T09:19:29.976809Z","iopub.status.idle":"2023-04-22T09:19:52.355488Z","shell.execute_reply.started":"2023-04-22T09:19:29.976764Z","shell.execute_reply":"2023-04-22T09:19:52.354181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gradient Boosting:","metadata":{}},{"cell_type":"markdown","source":"It is very slow and scores are worse, so I wouldn't advise you to use Gradient Boosting here","metadata":{}},{"cell_type":"code","source":"\"\"\"\nfrom xgboost import XGBRegressor\n\n# Create an instance of the XGBRegressor class\nxgb = XGBRegressor()\n\n# Define the hyperparameter grid\nparam_grid = {'n_estimators': [100, 500, 1000],\n              'max_depth': [3, 5, 7],\n              'learning_rate': [0.01, 0.1, 1.0]}\n\n# Perform a grid search to find the best hyperparameters\ngrid_search = GridSearchCV(xgb, param_grid, cv=5)\ngrid_search.fit(X_train_scaled, y_train)\n\n# Print the best hyperparameters\nprint(\"Best hyperparameters:\", grid_search.best_params_)\n\n# Train the XGBoost model with the best hyperparameters\nxgb_best = XGBRegressor(**grid_search.best_params_)\nxgb_best.fit(X_train_scaled, y_train)\n\n# Evaluate the model on the validation set\nval_pred = xgb_best.predict(X_val_scaled)\nval_score = r2_score(y_val, val_pred)\nprint(\"Validation R^2 score:\", val_score)\n\n# Evaluate the model on the test set\ntest_pred = xgb_best.predict(X_test_scaled)\ntest_score = r2_score(y_test, test_pred)\nprint(\"Test R^2 score:\", test_score)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-04-22T09:19:52.356995Z","iopub.execute_input":"2023-04-22T09:19:52.357343Z","iopub.status.idle":"2023-04-22T09:24:25.477137Z","shell.execute_reply.started":"2023-04-22T09:19:52.357297Z","shell.execute_reply":"2023-04-22T09:24:25.476052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion:\n\nBest result was achivied by Decision Tree. Sometimes it might Random Forest. \n\nFor datasets like this one it may be better to use a simpler model, such as a decision tree or a random forest, that can handle a small amount of data without overfitting. They do not require the removal of highly correlated features, and they can be a good choice of algorithm for datasets with multicollinearity issues.\n\nIn summary, decision trees and random forest are a good choice for small datasets (500x9) with highly correlated features. ","metadata":{}}]}